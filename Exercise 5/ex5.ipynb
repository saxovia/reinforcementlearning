{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9b29c9b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24cdaedcd4b5dfe347328615bb32c2c1",
     "grade": false,
     "grade_id": "cell-afd59c116a3ed810",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "   <h2 align=\"center\"> <center><b> Reinforcement Learning Assignment 5 - Policy Gradient </b></center></h2>\n",
    "\n",
    "<br>\n",
    "<center><font size=\"3\">This notebook is a part of teaching material for ELEC-E8125</font></center>\n",
    "<center><font size=\"3\">Sep 4, 2024 - Nov 30, 2024</font></center>\n",
    "<center><font size=\"3\">Aalto University</font></center>\n",
    "</div>\n",
    "\n",
    "\n",
    "<a id='TOC'></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    " \n",
    "# Table of contents\n",
    "* <a href='#1.'> 1. Introduction </a>\n",
    "* <a href='#1.1'> 1.1 Learning Objectives </a>\n",
    "* <a href='#1.2'> 1.2 Code Structure & Files </a>\n",
    "* <a href='#2.'> 2. Policy gradient with and without baseline</a>\n",
    "* <a href='#2.1'> 2.1 Training Loop</a>\n",
    "* <a href='#2.2'> 2.2 Policy with Fixed Variance </a>\n",
    "* <a href='#2.3'> 2.3 PG Objects</a>\n",
    "* <a href='#3.'> 3. Choosing the value of variance </a>\n",
    "* <a href='#3.1'> 3.1 Policy with Learned Variance </a>\n",
    "* <a href='#4.'> 4. PG and experience replay </a>\n",
    "* <a href='#5.'> 5. Real-world control problems </a>\n",
    "* <a href='#6.'> 6. Discrete action spaces </a>\n",
    "* <a href='#7.'> 7. Submitting </a>\n",
    "* <a href='#7.1'> 7.1 Feedback </a>\n",
    "* <a href='#8.'> References</a>\n",
    "\n",
    "<a href='#T1'><b>Student Task 1.</b> Implementing Reinforce PG algorithm </a>\\\n",
    "<a href='#T1a'><b>Student Task 1a.</b> Basic REINFORCE without baseline (15 points) </a>\\\n",
    "<a href='#T1b'><b>Student Task 1b.</b> REINFORCE with a constant baseline b = 20 (5 points) </a>\\\n",
    "<a href='#T1c'><b>Student Task 1c.</b> REINFORCE with discounted rewards normalized to zero mean and unit variance (5 points) </a>\\\n",
    "<a href='#Q1'><b>Student Question 1.1</b> A good baseline (15 points) </a>\\\n",
    "<a href='#T2'><b>Student Task 2.</b> Making Variance a Learnable Parameter (10 points) </a>\\\n",
    "<a href='#Q2'><b>Student Question 2.1</b> Constant vs Learnable Variance (5 points) </a>\\\n",
    "<a href='#Q3'><b>Student Question 2.2</b> Learnable Variance Initial Performance (5 points) </a> \\\n",
    "<a href='#Q4'><b>Student Question 3.</b> Considering a experience reply buffer (15 points)</a>\\\n",
    "<a href='#Q5'><b>Student Question 4.1</b> Considering an unbounded continuous action space part 1(5 points) </a>\\\n",
    "<a href='#Q6'><b>Student Question 4.2</b> Considering an unbounded continuous action space part 2 (10 points) </a>\\\n",
    "<a href='#Q7'><b>Student Question 5.</b> Considering discrete action spaces (10 points)</a>\n",
    "    \n",
    "**Total Points:** 100\n",
    "\n",
    "**Estimated runtime of all the cells:** 2 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c043fb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82ec0ae01bfbd06b245720b831476b14",
     "grade": false,
     "grade_id": "cell-ad52c038bf5d1c50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 1. Introduction <a id='1.'></a>\n",
    "In this exercise, we will implement the REINFORCE policy gradient algorithm for [InvertedPendulum environment](https://gymnasium.farama.org/environments/mujoco/inverted_pendulum/). The InvertedPendulum is similar to the previously used CartPole environment, except it has a continuous action space: the action can now have values in a range of [‚àí3,3]. The action value represents a numerical force applied to the cart, with magnitude representing the amount of force and sign representing the direction.\n",
    "\n",
    "## 1.2 Learning Objectives <a id='1.2'></a>\n",
    "- To understand how policy gradient works\n",
    "- To implement the REINFORCE policy gradient algorithm\n",
    "- To understand the limits and use cases of policy gradient\n",
    "\n",
    "## 1.3 Code Structure and Files <a id='1.3'></a>\n",
    "You don‚Äôt have to edit any other file other than ```ex5.ipynb``` to complete this exercise.\n",
    "\n",
    "```\n",
    "‚îú‚îÄ‚îÄ‚îÄcfg                  # Config files for environments e.g. define the maximum number of steps in an episode.\n",
    "‚îú‚îÄ‚îÄ‚îÄimgs                 # Images used in notebook\n",
    "‚îú‚îÄ‚îÄ‚îÄresults \n",
    "‚îÇ   ‚îú‚îÄ‚îÄ‚îÄlogging          # Contains logged data\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ‚îÄmodel            # Contains the policies learned\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄvideo            # Contains videos for each environment\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ‚îÄInvertedPendulum-v4\n",
    "‚îÇ       ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄtest      # Videos saved during testing\n",
    "‚îÇ       ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄtrain     # Videos saved during training\n",
    "‚îÇ   ex5.ipynb            # Main assignment file containing tasks <---------\n",
    "‚îÇ   setup.py             # Contains setup function\n",
    "‚îÇ   utils.py             # Contains useful functions \n",
    "```\n",
    "---\n",
    "The foundational class, `PG`, stands as the base class for the Policy Gradient. Three additional classes - `PGNoBase(PG)`, `PGBase20(PG)`, and `PGGamma(PG)` - inherit from this base class. Each class corresponds to a different variant of the policy gradient:\n",
    "\n",
    "- `PGNoBase(PG)`: Implements the vanilla Policy Gradient without a baseline.\n",
    "- `PGBase20(PG)`: Utilizes a baseline of 20.\n",
    "- `PGGamma(PG)`: Employs PG gamma with discounted rewards normalized to zero mean and unit variance.\n",
    "\n",
    "# 2. Policy gradient with and without baseline <a id='2.'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298df828",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e9d507428e153788d3bffa9d165dc4e",
     "grade": false,
     "grade_id": "cell-46a132ec9dcfe4ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Warnings:\n",
    "\n",
    "- Don‚Äôt copy and paste cells within a notebook. This will mess up the tracking metadata and prevent autograding from working.\n",
    "- Only add new cells using the '+' button in the upper toolbar and do not split cells.\n",
    "- Be cautious about things such as copying the whole notebook to Colab to work on it. This has sometimes resulted in removing all notebook metadata, making autograding impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabd7b5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46cf9e8cd6b3dfa2c69dcf9ebd86720f",
     "grade": false,
     "grade_id": "cell-f93aed21b4a0d27f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='T1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1.</b> Implementing Reinforce PG algorithm (25 points) </h3> \n",
    "\n",
    "Implement the REINFORCE policy gradient algorithm to balance the InvertedPendulum. \n",
    "<br> Use constant standard deviation $\\sigma = 1$ (i.e. $\\log(\\sigma) = 0$) for the output action distribution throughout the training. Implement:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (a) basic REINFORCE without baseline (15 points),\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (b) REINFORCE with a constant baseline b = 20 (5 points),\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (c) REINFORCE with discounted rewards normalized to zero mean and unit variance (5 points),\n",
    "    \n",
    "**Complete the all the unfinished implementation in following cells (marked with ```TODOs```)**. \n",
    "    \n",
    "1. **Train Loop Function**: Finalize the `train_iteration(agent, env)` function to establish the training loop, similarly to how it was done in Exercise 1.\n",
    "2. **Policy Class with Fixed Variance**: Complete the `PolicyFixedVar(nn.Module)` class to generate a policy with a fixed variance.\n",
    "3. **Get Action Method**: Finish the `get_action(self, observation, evaluation=False)` method within the `PG` class, ensuring it can be inherited effectively.\n",
    "4. **Complete PG Classes**: Implepment the `PGNoBase(PG)`, `PGBase20(PG)`, and `PGGamma(PG)` classes, each tailored with a different baseline.\n",
    "\n",
    "**Hint:** The ```class PolicyFixedVar(nn.Module)```  contains a basic neural network structure. And we include reasonable\n",
    "hyperparameters in the ```cfg``` folder.\n",
    "\n",
    "**Hint:** Your policy should output a probability distribution over actions. A suitable and straightforward choice is to use a normal distribution (`from torch.distributions import Normal`). Compute log probabilities using the `log_prob` function of the distribution. We strongly recommend you to read the official PyTorch documentation to learn how to use the distributions and related functions.\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca4ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = False  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9140b77c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "414e47f15fa3b52031225c1d85f7c4ad",
     "grade": true,
     "grade_id": "cell-b087b63d5c88b12c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46471b1e-91d6-48bb-bb0d-f3de4be1aa79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing system and path-related modules\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "# Importing PyTorch related modules\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Importing other third-party modules\n",
    "import numpy as np\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from IPython.display import Video  # to display videos\n",
    "\n",
    "# Importing local utility modules\n",
    "import utils as u\n",
    "from setup import setup\n",
    "\n",
    "# Setting the device for storing tensors/calculations\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55e45ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1 Training Loop <a id='2.1'></a>\n",
    "\n",
    "In the subsequent cell, a foundational framework is provided for training a Reinforcement Learning (RL) agent employing the REINFORCE algorithm. It is constructed to efficiently monitor the agent's progress and performance throughout the training phase. Please complete the training loop, marked with `TODO`, to finalize the setup for the agent‚Äôs training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ff872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    \"\"\"\n",
    "    Convert a PyTorch tensor to a NumPy array.\n",
    "\n",
    "    :param tensor: A PyTorch tensor.\n",
    "    :return: The converted NumPy array.\n",
    "    \"\"\"\n",
    "    # Squeeze the tensor, move it to CPU and convert to NumPy array\n",
    "    return tensor.squeeze(0).cpu().numpy()\n",
    "\n",
    "\n",
    "def train_iteration(agent, env):\n",
    "    # Policy training function\n",
    "\n",
    "    # Reset the environment and observe the initial state\n",
    "    reward_sum, timesteps, done = 0, 0, False\n",
    "    obs, _ = env.reset(seed=409)\n",
    "\n",
    "    while not done:\n",
    "        # TODO: Task 1: finish the train loop, including \n",
    "        # 1. Call agent.get_action to get action and log prob of the action\n",
    "        # 2. Call env.step with the action (note: you need to convert the action into a numpy array -- use the function to_numpy for this)\n",
    "        #   (Steps 1. and 2. you can also find from the 'test' function below)\n",
    "        # 3. Store the log prob of action and reward by calling agent.record\n",
    "        # 4. Update reward_sum by adding the reward received from env.step, and increase timesteps by one\n",
    "        # 5. Use the observation you receive from env.step to call agent.get_action for the next timestep \n",
    "        #   (Hint: ensure to use the `.copy()` method to avoid unexpected behavior due to aliasing.)\n",
    "\n",
    "        ########### Your code starts here ###########\n",
    "        # Get action from the agent\n",
    "\n",
    "        # Perform the action on the environment, get new state and reward\n",
    "    \n",
    "        # Store agent's and env's outcome (so that the agent can improve its policy)\n",
    "\n",
    "        # Store total episode reward\n",
    " \n",
    "        # update observation\n",
    "\n",
    "        ########## Your codes ends here. ##########\n",
    "        if timesteps >= 1000:\n",
    "            done = True\n",
    "            \n",
    "    # Update the policy after one episode\n",
    "    info = agent.update()\n",
    "\n",
    "    # Return stats of training\n",
    "    info.update({'timesteps': timesteps,\n",
    "                'ep_reward': reward_sum,})\n",
    "    return info\n",
    "\n",
    "\n",
    "def train(agent_class, cfg_path, cfg_args={}):\n",
    "    env, cfg = setup(cfg_path, cfg_args=cfg_args)\n",
    "    performance_data = []\n",
    "    \n",
    "    if cfg.save_logging: \n",
    "        u.make_dir(Path().cwd()/'results'/cfg.env_name/\"logging\")\n",
    "        L = u.Logger() # create a simple logger to record stats\n",
    "        \n",
    "    # Get state and action dimensionality\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "    # Initialise the policy gradient agent\n",
    "    agent = agent_class(state_dim, action_dim, float(cfg.lr), float(cfg.gamma))\n",
    "    \n",
    "    for ep in range(cfg.train_episodes):\n",
    "        # collect data and update the policy\n",
    "        train_info = train_iteration(agent, env)\n",
    "        train_info.update({'episodes': ep})\n",
    "\n",
    "        if cfg.save_logging:\n",
    "            L.log(**train_info)\n",
    "        if (not cfg.silent) and (ep % 100 == 0):\n",
    "            print({\"ep\": ep, **train_info})\n",
    "\n",
    "        performance_data.append(train_info)\n",
    "        \n",
    "    u.plot_training_data(performance_data)\n",
    "    if cfg.save_model:\n",
    "        agent.save(cfg.model_path)\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "\n",
    "# Function to test a trained policy\n",
    "@torch.no_grad()\n",
    "def test(agent_class, cfg_path, cfg_args={}, num_episodes=10, seeds=None):\n",
    "    env, cfg = setup(cfg_path, cfg_args=cfg_args)\n",
    "    \n",
    "    # Get state and action dimensionality\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "    # Initialise the policy gradient agent\n",
    "    agent = agent_class(state_dim, action_dim, float(cfg.lr), float(cfg.gamma))\n",
    "    \n",
    "    print(\"Loading model from\", cfg.model_path, \"...\")\n",
    "    agent.load(cfg.model_path)\n",
    "        \n",
    "    test_rewards = []\n",
    "    for ep in range(num_episodes):\n",
    "\n",
    "        if seeds != None:\n",
    "            test_seed = seeds[ep % len(seeds)]\n",
    "            print(\"Episode seed: \", test_seed)\n",
    "            (obs, _)  = env.reset(seed=test_seed)\n",
    "        else :\n",
    "            (obs, _)  = env.reset()\n",
    "        done=  False\n",
    "        test_reward = 0\n",
    "        timesteps = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Similar to the training loop above -\n",
    "            # get the action, act on the environment, save total reward\n",
    "            # (evaluation=True makes the agent always return what it thinks to be\n",
    "            # the best action - there is no exploration at this point)\n",
    "            action, _ = agent.get_action(obs, evaluation=True)\n",
    "\n",
    "            \n",
    "            obs, reward, done, _, info = env.step(to_numpy(action))\n",
    "            timesteps += 1\n",
    "            test_reward += reward\n",
    "            \n",
    "            if timesteps >= 1000:\n",
    "                done = True\n",
    "        test_rewards.append(test_reward)\n",
    "\n",
    "        print(\"Test ep_reward:\", test_reward)\n",
    "\n",
    "    print(\"Median test reward:\", np.median(test_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac054ea1",
   "metadata": {},
   "source": [
    "## 2.2 Policy with Fixed Variance <a id='2.2'></a>\n",
    "\n",
    "This code is designed to establish a neural network policy where the variance is constant. The `PolicyFixedVar` class, which is a subclass of `nn.Module`, defines this policy. The policy uses a neural network to map states to actions, outputting a normal distribution with a mean and a standard deviation. \n",
    "\n",
    "#### Task:\n",
    "1. **Implementing actor_logstd:**\n",
    "   - Initialize `self.actor_logstd` as a torch tensor with zeros, ensuring to set the `device` parameter for appropriate storage.\n",
    "\n",
    "2. **Create and Return Distribution:**\n",
    "   - In the `forward` method, create a Normal distribution (`probs`) with the computed `action_mean` and `action_std`. This distribution represents the policy‚Äôs output given a state input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a5cc6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialisation function for neural network layers\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "# This class defines the neural network policy\n",
    "class PolicyFixedVar(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyFixedVar, self).__init__()\n",
    "\n",
    "        # Initialise a neural network with two hidden layers (64 neurons per layer)\n",
    "        self.actor_mean = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_dim, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, action_dim), std=0.01),\n",
    "        )\n",
    "\n",
    "        # TODO: Task 1: Implement actor_logstd as a torch tensor. \n",
    "        # Hint: when creating the torch tensor, remember to set the parameter device=device \n",
    "        #       so the tensor is stored correctly on CUDA (if applicable) \n",
    "        ########## Your code starts here. ###########\n",
    "        \n",
    "        ########## Your code ends here. ##########\n",
    "\n",
    "    # Do a forward pass to map state to action\n",
    "    def forward(self, state):\n",
    "        # Get mean of a Normal distribution (the output of the neural network)\n",
    "        action_mean = self.actor_mean(state)\n",
    "\n",
    "        # Make sure action_logstd matches dimension of action_mean\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "\n",
    "        # Exponentiate the log std to get actual std\n",
    "        action_std = torch.exp(action_logstd)\n",
    "\n",
    "        # TODO: Task 1: Create a Normal distribution with mean of 'action_mean' and standard deviation of 'action_std', and return the distribution\n",
    "        # Use the torch.distributions class\n",
    "        ########## Your code starts here. ###########\n",
    "\n",
    "        ########## Your code ends here. ##########\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b677a295",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e673d7a1cb02204c75f9b5ea389b7d84",
     "grade": true,
     "grade_id": "cell-8e7d3a4799495ca0",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Policy\n",
    "def test_return_class():\n",
    "    policy = PolicyFixedVar(4, 1)\n",
    "    x = torch.Tensor((1.,2.,3.,4.)).reshape((1,4))\n",
    "    assert isinstance( policy.forward(x),Normal)\n",
    "\n",
    "test_return_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de05f7e",
   "metadata": {},
   "source": [
    "## 2.3 PG Objects <a id='2.3'></a>\n",
    "\n",
    "The `PG` class is defined as the Policy Gradient base class. This class forms the foundation for implementing policy gradient algorithms, providing a structured format and placeholders for essential functions and variables used in these algorithms. The methods within this class will be used to interact with, update, and manage the policy used by the agent to make decisions in the environment.\n",
    "\n",
    "Task: Implementing the `get_action` Method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e300959",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Class for the Policy Gradient algorithm\n",
    "class PG(object):\n",
    "    def __init__(self, state_dim, action_dim, lr, gamma):\n",
    "        # Define the neural network policy with fixed variance \n",
    "        self.policy = PolicyFixedVar(state_dim, action_dim).to(device)\n",
    "\n",
    "        # Create an optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "\n",
    "        # Set discount factor value\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Simple buffers for action probabilities and rewards\n",
    "        self.action_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "    def record(self, action_prob, reward):\n",
    "        \"\"\" Store agent's and env's outcomes to update the agent.\"\"\"\n",
    "        self.action_probs.append(action_prob)\n",
    "        self.rewards.append(torch.tensor([reward]))\n",
    "\n",
    "    def save(self, filepath):\n",
    "        torch.save(self.policy.state_dict(), filepath)\n",
    "\n",
    "    def load(self, filepath):\n",
    "        self.policy.load_state_dict(torch.load(filepath))\n",
    "    \n",
    "    def get_action(self, observation, evaluation=False):\n",
    "        \"\"\"Return action and logprob of this action.\"\"\"\n",
    "\n",
    "        # Add batch dimension if necessary\n",
    "        if observation.ndim == 1: \n",
    "            observation = observation[None]\n",
    "\n",
    "        # Convert observation to a torch tensor\n",
    "        x = torch.from_numpy(observation).float().to(device)\n",
    "\n",
    "        # TODO: Task 1: Calculate action and its log_prob\n",
    "        ########## Your code starts here. ###########\n",
    "        # Hint: \n",
    "        #   1. when evaluation=True, return mean, otherwise return samples from the distribution created in self.policy.forward() function.\n",
    "        #   2. notice the shape of action and act_logprob.\n",
    "        \n",
    "        # Pass state x through the policy network (T1)\n",
    "\n",
    "        # Return mean if evaluation, else sample from the distribution\n",
    "\n",
    "        # Calculate the log probability of the action (T1)\n",
    "        \n",
    "        ########## Your code ends here. ##########\n",
    "        \n",
    "        if observation.ndim == 1: \n",
    "            action = action[0]\n",
    "\n",
    "        return action, act_logprob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61981975",
   "metadata": {},
   "source": [
    "<a id='T1a'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1a.</b> Basic REINFORCE without baseline (15 points) </h3> \n",
    "The training performance plot might look like Figure 1a. \n",
    "    <figure style=\"text-align: center\">\n",
    "    <img src=\"imgs/PGNoBase.svg\" width=\"600px\">\n",
    "    <figcaption style=\"text-align: center\"> Figure 1a: REINFORCE without baseline. </figcaption>\n",
    "    </figure>\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e164da7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PGNoBase(PG):\n",
    "    def __init__(self, state_dim, action_dim, lr, gamma):\n",
    "        super().__init__(state_dim, action_dim, lr, gamma)\n",
    "\n",
    "    # TODO: Task 1: Implement the policy gradient\n",
    "    # Complete the following 2 functions\n",
    "    # Hints:\n",
    "    #   1. compute discounted rewards (use the discount_rewards function offered in common.helper)\n",
    "    #   2. compute the policy gradient loss\n",
    "    \n",
    "    # Compute discounted rewards \n",
    "    def calculate_discounted_rewards(self, rewards):\n",
    "        ########## Your code starts here. ##########\n",
    "        \n",
    "        ########## Your code ends here. ##########\n",
    "        return discounted_rewards\n",
    "\n",
    "    # Calculate the PG loss\n",
    "    def calculate_loss(self, action_probs, discounted_rewards):\n",
    "        ########## Your code starts here. ##########\n",
    "\n",
    "        ########## Your code ends here. ##########\n",
    "        return loss\n",
    "    \n",
    "    def update(self):\n",
    "        # Prepare dataset used to update policy\n",
    "        action_probs = torch.stack(self.action_probs, dim=0).to(device).squeeze(-1) # shape: [batch_size,]\n",
    "        rewards = torch.stack(self.rewards, dim=0).to(device).squeeze(-1) # shape [batch_size,]\n",
    "        self.action_probs, self.rewards = [], [] # clean buffers\n",
    "        \n",
    "        # Task 1: Implement the policy gradient\n",
    "        \n",
    "        # Compute discounted rewards \n",
    "        discounted_rewards = self.calculate_discounted_rewards(rewards)\n",
    "\n",
    "        # Calculate the PG loss\n",
    "        loss = self.calculate_loss(action_probs, discounted_rewards)\n",
    "       \n",
    "        # Backprop gradients\n",
    "        loss.backward()  \n",
    "\n",
    "        # Do the optimizer step \n",
    "        self.optimizer.step() \n",
    "        self.optimizer.zero_grad() \n",
    "\n",
    "        # if you want to log something in wandb, you can put them inside the {}, otherwise, just leave it empty.\n",
    "        return {'logstd': self.policy.actor_logstd.cpu().detach().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63656ac2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58227601cec76e5daee3bee9f5cb8f0c",
     "grade": true,
     "grade_id": "cell-170a359e31be5be6",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST CELL\"\n",
    "#Test get_action method\n",
    "def test_get_action_1():\n",
    "    cfg_path=Path().cwd()/'cfg'/'inverted_pendulum.yaml'\n",
    "    env, cfg = setup(cfg_path, cfg_args={\"model_name\":\"PG\",\"seed\": 43})\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    agent = PG(state_dim, action_dim, float(cfg.lr), float(cfg.gamma))\n",
    "    obs = np.array( [0.00565713,  0.00385536, -0.09085017,  0.20961052] )\n",
    "    action = -0.0009746950818225741\n",
    "    assert np.allclose(agent.get_action(obs,True)[0].item(),action,0.001)\n",
    "    \n",
    "    agent2 = PG(state_dim, 3, float(cfg.lr), float(cfg.gamma))\n",
    "    assert tuple(agent2.get_action(obs,True)[1].shape) == (1,)\n",
    "\n",
    "test_get_action_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27e7ee9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e264785d47b98f7c755dac88b8a7a3d",
     "grade": true,
     "grade_id": "cell-01beb3c8be62ffc0",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST CELL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f72fee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    train(PGNoBase, cfg_path=Path().cwd() / 'cfg' / 'inverted_pendulum.yaml', cfg_args=dict(model_name=\"PGNoBase\")) # < 7 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ab5a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    test(PGNoBase, cfg_path=Path().cwd() / 'cfg' / 'inverted_pendulum.yaml', cfg_args=dict(model_name=\"PGNoBase\", save_video=True, testing=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2714f3db",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c2e40d8dc91bbf04c0c774bcec4807f",
     "grade": true,
     "grade_id": "cell-88ff36aa060ed0d0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5178279",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1773f5d1773107777dffbcfb71019cf",
     "grade": true,
     "grade_id": "cell-9efc8886a62a7f68",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST CELL\"\n",
    "def test_pgnobase_update_1():\n",
    "    agent_class = PGNoBase\n",
    "    cfg_path=Path().cwd() / 'cfg' / 'inverted_pendulum.yaml'\n",
    "    cfg_args=dict(model_name=\"PGNoBase\", seed=43)\n",
    "    env, cfg = setup(cfg_path, cfg_args=cfg_args)\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "    agent = agent_class(state_dim, action_dim, float(cfg.lr), float(cfg.gamma))\n",
    "\n",
    "    test_reward = torch.Tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
    "    test_action_prob = torch.Tensor([-2.4205, -2.1635, -1.2791, -1.2655, -0.9205, -0.9366, -4.9679, -2.1623])\n",
    "    test_discounted_reward = torch.Tensor([7.7255, 6.7935, 5.8520, 4.9010, 3.9404, 2.9701, 1.9900, 1.0000])\n",
    "\n",
    "    assert torch.allclose(agent.calculate_discounted_rewards(test_reward), test_discounted_reward, 0.001)\n",
    "    assert torch.allclose(agent.calculate_loss(test_action_prob, test_discounted_reward), torch.tensor(8.1928), 0.001)\n",
    "\n",
    "test_pgnobase_update_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a39f4ea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a71971cc15d36173098a756d78fa1f2e",
     "grade": true,
     "grade_id": "cell-327283bf9249663a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST CELL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff999fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "      video = Video(Path().cwd()/'results'/'InvertedPendulum-v4'/'video'/'test'/'ex5-episode-5.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control\n",
    "      display(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a2410-d8ea-4e1f-b10a-e112816124e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "      video = Video(Path().cwd()/'results'/'InvertedPendulum-v4'/'video'/'test'/'ex5-episode-0.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control\n",
    "      display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea079dc7",
   "metadata": {},
   "source": [
    "<a id='T1b'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1b.</b> REINFORCE with a constant baseline b = 20 (5 points) </h3> \n",
    "    The training performance plot might look like Figure 1b.\n",
    "    <figure style=\"text-align: center\" id=\"figure-1b\">\n",
    "        <img src=\"imgs/PGBase20.svg\" width=\"600px\">\n",
    "        <figcaption style=\"text-align: center\"> Figure 1b: REINFORCE with a constant baseline 20. </figcaption>\n",
    "    </figure>\n",
    "    \n",
    "üîù <a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c57e167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PGBase20(PG):\n",
    "    def __init__(self, state_dim, action_dim, lr, gamma):\n",
    "        super().__init__(state_dim, action_dim, lr, gamma)\n",
    "\n",
    "    # TODO: Task 1: Implement the policy gradient\n",
    "    # Complete the following 2 functions\n",
    "    # Hints:\n",
    "    #   1. compute discounted rewards (use the discount_rewards function offered in common.helper)\n",
    "    #   2. compute the policy gradient loss with the baseline\n",
    "    \n",
    "    # Compute discounted rewards \n",
    "    def calculate_discounted_rewards(self, rewards):\n",
    "        ########## Your code starts here. ##########\n",
    "        \n",
    "        ########## Your code ends here. ##########\n",
    "        return discounted_rewards\n",
    "\n",
    "    # Calculate the PG loss with the baseline\n",
    "    def calculate_loss(self, action_probs, discounted_rewards):\n",
    "        ########## Your code starts here. ##########\n",
    "\n",
    "        ########## Your code ends here. ##########\n",
    "        return loss\n",
    "\n",
    "    def update(self):\n",
    "        # Prepare dataset used to update policy\n",
    "        action_probs = torch.stack(self.action_probs, dim=0).to(device).squeeze(-1) # shape: [batch_size,]\n",
    "        rewards = torch.stack(self.rewards, dim=0).to(device).squeeze(-1) # shape [batch_size,]\n",
    "        self.action_probs, self.rewards = [], [] # clean buffers\n",
    "        \n",
    "        # TODO: Task 1: Implement the policy gradient\n",
    "\n",
    "        # Compute discounted rewards \n",
    "        discounted_rewards = self.calculate_discounted_rewards(rewards)\n",
    "        # Calculate the PG loss with the baseline\n",
    "        loss = self.calculate_loss(action_probs, discounted_rewards)\n",
    "        # Backprop gradients\n",
    "        loss.backward()\n",
    "        # Do the optimizer step\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # if you want to log something in wandb, you can put them inside the {}, otherwise, just leave it empty.\n",
    "        return {'logstd': self.policy.actor_logstd.cpu().detach().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa4d283",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    train(PGBase20, cfg_path=Path().cwd() / 'cfg' / 'inverted_pendulum.yaml', cfg_args=dict(model_name=\"PGBase20\")) # < 7 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f26f40-49c9-48f3-8543-cae81be8185f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    test(PGBase20, cfg_path=Path().cwd() / 'cfg' / 'inverted_pendulum.yaml', cfg_args=dict(model_name=\"PGBase20\", save_video=True, testing=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4ac907",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e67902311730193101e4741111bbf0b2",
     "grade": true,
     "grade_id": "cell-62d1b98f11dcd94b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6074d987",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11f6f2b5baa3638d6d0deedc0eee29fe",
     "grade": true,
     "grade_id": "cell-3bcb1eace5908e1d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST CELL\"\n",
    "def test_pgbase20_update_1():\n",
    "    \n",
    "    agent_class = PGBase20\n",
    "    cfg_path=Path().cwd() / 'cfg' / 'inverted_pendulum.yaml'\n",
    "    cfg_args=dict(model_name=\"PGBase20\", seed=43)\n",
    "    env, cfg = setup(cfg_path, cfg_args=cfg_args)\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "    agent = agent_class(state_dim, action_dim, float(cfg.lr), float(cfg.gamma))\n",
    "\n",
    "    test_reward = torch.Tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
    "    test_action_prob = torch.Tensor([-2.4205, -2.1635, -1.2791, -1.2655, -0.9205, -0.9366, -4.9679, -2.1623])\n",
    "    test_discounted_reward = torch.Tensor([7.7255, 6.7935, 5.8520, 4.9010, 3.9404, 2.9701, 1.9900, 1.0000])\n",
    "\n",
    "    assert torch.allclose(agent.calculate_discounted_rewards(test_reward), test_discounted_reward, 0.001)\n",
    "    assert torch.allclose(agent.calculate_loss(test_action_prob, test_discounted_reward), torch.tensor(-32.0970), 0.001)\n",
    "\n",
    "test_pgbase20_update_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42de046f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd3eaf1021b2d35e4bac9eb0a6515439",
     "grade": true,
     "grade_id": "cell-7c3853783c05c900",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST CELL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a434d9ef-523c-482a-a2e8-441ac66e15e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "      video = Video(Path().cwd() / 'results' / 'InvertedPendulum-v4' / 'video' / 'test' / 'ex5-episode-0.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control\n",
    "      display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c7d2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='T1c'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 1c.</b> REINFORCE with discounted rewards normalized to zero mean and unit variance (5 points) </h3> \n",
    "    The training performance plot might look like Figure 1c. \n",
    "    <figure style=\"text-align: center\" id=\"figure-1c\">\n",
    "        <img src=\"imgs/PGGamma.svg\" width=\"600px\">\n",
    "        <figcaption style=\"text-align: center\"> Figure 1c: REINFORCE with discounted rewards normalized to zero mean and unit variance. </figcaption>\n",
    "    </figure>\n",
    "    \n",
    "üîù <a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76bcc3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PGNormalized(PG):\n",
    "    def __init__(self, state_dim, action_dim, lr, gamma):\n",
    "        super().__init__(state_dim, action_dim, lr, gamma)\n",
    "\n",
    "    # TODO: Task 1: Implement the policy gradient\n",
    "    # Complete the following 2 functions\n",
    "    # Hints:\n",
    "    #   1. compute discounted rewards (use the discount_rewards function offered in utils.py)\n",
    "    #   2. compute the policy gradient loss\n",
    "    \n",
    "    # Compute discounted rewards \n",
    "    def calculate_discounted_rewards(self, rewards):\n",
    "        ########## Your code starts here. ##########\n",
    "        \n",
    "        ########## Your code ends here. ##########\n",
    "        return discounted_rewards\n",
    "\n",
    "    # Calculate the PG loss with the baseline\n",
    "    def calculate_loss(self, action_probs, discounted_rewards):\n",
    "        ########## Your code starts here. ##########\n",
    "\n",
    "        ########## Your code ends here. ##########\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        # Prepare dataset used to update policy\n",
    "        action_probs = torch.stack(self.action_probs, dim=0).to(device).squeeze(-1) # shape: [batch_size,]\n",
    "        rewards = torch.stack(self.rewards, dim=0).to(device).squeeze(-1) # shape [batch_size,]\n",
    "        self.action_probs, self.rewards = [], [] # clean buffers\n",
    "        \n",
    "        # TODO: Task 1: Implement the policy gradient\n",
    "        \n",
    "        # Compute discounted rewards and normalize them\n",
    "        discounted_rewards = self.calculate_discounted_rewards(rewards)\n",
    "        # Calculate the PG loss\n",
    "        loss = self.calculate_loss(action_probs, discounted_rewards)\n",
    "        # Backprop gradients\n",
    "        loss.backward()\n",
    "        # Do the optimizer step\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()    \n",
    "\n",
    "        # if you want to log something in wandb, you can put them inside the {}, otherwise, just leave it empty.\n",
    "        return {'logstd': self.policy.actor_logstd.cpu().detach().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e236519-8afb-4ec7-ba87-3181bf822576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    train(PGNormalized, cfg_path=Path().cwd() / 'cfg' / 'inverted_pendulum.yaml', cfg_args=dict(model_name=\"PGNormalized\")) # < 10 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab775f-bc7e-475f-ac41-7bbf0227bc1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    test(PGNormalized, cfg_path=Path().cwd() / 'cfg' / 'inverted_pendulum.yaml', cfg_args=dict(model_name=\"PGNormalized\", save_video=True, testing=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc850792",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2419c79f89c4ca692b669552b27e124",
     "grade": true,
     "grade_id": "cell-03e739e487933e6e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8fb1c0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "106d897f0caa69b7eb88435c2b1b3043",
     "grade": true,
     "grade_id": "cell-a90ba2f2bac40f9c",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST CELL\"\n",
    "def test_pgnormalized_update_1():\n",
    "    \n",
    "    agent_class = PGNormalized\n",
    "    cfg_path=Path().cwd() / 'cfg' / 'inverted_pendulum.yaml'\n",
    "    cfg_args=dict(model_name=\"PGNormalized\", seed=43)\n",
    "    env, cfg = setup(cfg_path, cfg_args=cfg_args)\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "    agent = agent_class(state_dim, action_dim, float(cfg.lr), float(cfg.gamma))\n",
    "\n",
    "    test_reward = torch.Tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
    "    test_action_prob = torch.Tensor([-2.4205, -2.1635, -1.2791, -1.2655, -0.9205, -0.9366, -4.9679, -2.1623])\n",
    "    test_discounted_reward = torch.Tensor([7.7255, 6.7935, 5.8520, 4.9010, 3.9404, 2.9701, 1.9900, 1.0000])\n",
    "\n",
    "    assert torch.allclose(agent.calculate_discounted_rewards(test_reward), test_discounted_reward, 0.001)\n",
    "    assert torch.allclose(agent.calculate_loss(test_action_prob, test_discounted_reward), torch.tensor(-0.2822), 0.001)\n",
    "\n",
    "test_pgnormalized_update_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36fae08",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d2fb6c9e677aa86435b3c7b06302512",
     "grade": true,
     "grade_id": "cell-a4862f110550460f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\"TEST CELL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3bbe30-5d8f-426d-9a76-806e6586a23b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "      video = Video(Path().cwd() / 'results' / 'InvertedPendulum-v4' / 'video' / 'test' / 'ex5-episode-0.mp4',\n",
    "      embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control\n",
    "      display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f856d17c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5344618e774be27b6e56107827fe9e38",
     "grade": false,
     "grade_id": "cell-970e68431aa3a7cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Q1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 1.1</b> A good baseline (15 points) </h3> \n",
    "\n",
    "Which of the following statements correctly describe how to choose a good baseline and why it makes training more stable? **Select 5 options.**\n",
    "            \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c46c994-d93c-4188-adef-ef82dabe437c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b1eed5810b36c7aaf8e100f5b08c77b2",
     "grade": false,
     "grade_id": "cell-cfe2a992cab77b22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Options:\n",
    "\n",
    "1. A good baseline doesn't affect the expectation (doesn't introduce bias) but reduces variance of the estimated gradients. \n",
    "2. The baseline should be dependent on both the state and the action to provide the most accurate estimates.\n",
    "3. The optimal baseline can be theoretically derived, but it is too expensive to calculate in practice. \n",
    "4.  A good baseline should be a constant value to ensure consistency across all states. \n",
    "5.  The lower variance estimates lead to more stable training. \n",
    "6.  A good choice of baseline is an estimate of the state value, or the empirical mean (when doing Monte Carlo roll-outs). \n",
    "7.  The idea with both is that we increase the probability of actions that lead to higher returns than what our current estimate of the expected return is. \n",
    "8.  A proper baseline eliminates the need for exploration in reinforcement learning algorithms. \n",
    "9.  The choice of baseline has no impact on the convergence speed of the algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71bb55e-e23e-48ee-b9d5-da004df72b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer question 1.1 with appropriate option numbers\n",
    "sq1_1 = [] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b0074d-e8b9-4106-9155-bcc44fafd967",
   "metadata": {},
   "source": [
    "The following cells are used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1438ee-92e4-4123-99ae-bd989bfe1b81",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "068bab37656d0f1f76a988a2d372cdbe",
     "grade": true,
     "grade_id": "cell-4710481b388d34ce",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c385229f-7736-4fb2-92d4-c2c62d7ae1ac",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fcbf26c4ff358b5eaf69cab8b563bc5",
     "grade": true,
     "grade_id": "cell-fdd8262ca17746aa",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431586b5-619b-43e8-9045-ad0f3f828b05",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c2bba5cef04af5c1c243114c256376d",
     "grade": true,
     "grade_id": "cell-a0a1d58190a1e0f1",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1699f5-73ef-4eac-9a5a-17708c1d6ccd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c576e4932e49fe2c4cf243edd4035915",
     "grade": true,
     "grade_id": "cell-3343cc12855e6f15",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d789840-2b3e-4a5b-ba87-f9cc01adb538",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1414b678f26986c875a98c0184021ba9",
     "grade": true,
     "grade_id": "cell-86090270df654ed6",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bda4d734",
   "metadata": {},
   "source": [
    "# 3. Choosing the value of variance <a id='3.'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34d6f9d",
   "metadata": {},
   "source": [
    "<a id='T2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task 2.</b> Making Variance a Learnable Parameter (10 points) </h3> \n",
    "\n",
    "Implement the policy‚Äôs variance as a learnable parameter of the network and update it during training. Set the initial value $\\sigma_0^2$ to 1. REINFORCE with normalized discounted returns is used for this task.\n",
    "\n",
    "**Complete the unfinished implementation in ```PolicyLearnedVar(nn.Module)``` class (marked with ```TODOs```)**. \n",
    "    \n",
    "    \n",
    "**Hint:** To make your learned variance automatically updated by the optimizer, declare your variable inside the ```__init__``` function of the model using ```torch.nn.Parameter(some_tensor)```. \n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc8f47f",
   "metadata": {},
   "source": [
    "## 3.1 Policy with Learned Variance <a id='3.1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f38d70b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialisation function for neural network layers\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "# This class defines the neural network policy with learned variance\n",
    "class PolicyLearnedVar(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyLearnedVar, self).__init__()\n",
    "\n",
    "        # Initialise a neural network with two hidden layers (64 neurons per layer)\n",
    "        self.actor_mean = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_dim, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, action_dim), std=0.01),\n",
    "        )\n",
    "\n",
    "        # TODO: Task 2: Implement actor_logstd as a learnable parameter\n",
    "        # Use log of std to make sure std doesn't become negative during training\n",
    "        ########## Your code starts here. ###########    \n",
    "\n",
    "        ########## Your code ends here. ##########\n",
    "        \n",
    "    # Do a forward pass to map state to action\n",
    "    def forward(self, state):\n",
    "        # Get mean of a Normal distribution (the output of the neural network)\n",
    "        action_mean = self.actor_mean(state)\n",
    "\n",
    "        # Make sure action_logstd matches dimension of action_mean\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "\n",
    "        # Exponentiate the log std to get actual std\n",
    "        action_std = torch.exp(action_logstd)\n",
    "\n",
    "        # TODO: Task 1: Create a Normal distribution with mean of 'action_mean' and standard deviation of 'action_std', and return the distribution\n",
    "        ########## Your code starts here. ###########\n",
    "\n",
    "        ########## Your code ends here. ##########\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe99408-3caa-43d6-aa52-652c9e1b88fe",
   "metadata": {},
   "source": [
    "Initialize the `PG` class to construct the neural network policy. Continue to employ the `get_action` function, as outlined in Task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304d7def-5ad3-461e-a2c6-216157833223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Class for the Policy Gradient algorithm\n",
    "class PGLearnedVar(PGNormalized):\n",
    "    def __init__(self, state_dim, action_dim, lr, gamma):\n",
    "        super().__init__(state_dim, action_dim, lr, gamma)\n",
    "        \n",
    "        # TODO: Task 2: Define the neural network policy (self.policy) with learned variance.\n",
    "        self.policy = None\n",
    "        ########## Your code starts here. ###########  \n",
    "\n",
    "        ########## Your code ends here. ##########\n",
    "\n",
    "        # Create an optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7c5cac-1e2f-4224-a503-bc1e27871a09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    train(PGLearnedVar, cfg_path=Path().cwd() / 'cfg' / 'inverted_pendulum.yaml', cfg_args=dict(model_name=\"PGLearnedVar\")) # < 10 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d4bbd-9d79-4e23-a92a-d593a192a2c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    test(PGLearnedVar, cfg_path=Path().cwd() / 'cfg' / 'inverted_pendulum.yaml', cfg_args=dict(model_name=\"PGLearnedVar\", save_video=True, testing=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afce7ed",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "488b0594c6a0bc7114212e08886456da",
     "grade": true,
     "grade_id": "cell-b30e0b4e722242a6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c765232f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ab5d3ddd4f03fbc59870f78a083f9bf",
     "grade": true,
     "grade_id": "cell-cfe147d31b546e45",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da748d77-46d5-4597-b2f9-d9885ca24e6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    video = Video(Path().cwd() / 'results' / 'InvertedPendulum-v4' / 'video' / 'test' / 'ex5-episode-0.mp4',\n",
    "    embed=True, html_attributes=\"loop autoplay\") # Set html_attributes=\"controls\" for video control\n",
    "    display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3294a4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fab39c6fa2607a23e24d1db8c8081c5a",
     "grade": false,
     "grade_id": "cell-f5cbd8946325f4b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Q2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 2.1</b> Constant vs Learnable Variance (5 points) </h3> \n",
    "\n",
    "Which of the following statements accurately describe the strengths and weaknesses of using constant variance vs. learning the variance during training? **Select 5 options.**      \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc0af92-69e2-4092-8e09-d070351e2fde",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f9b59308ffd6714780a24779a21a972",
     "grade": false,
     "grade_id": "cell-5e9bfdef15f9a0fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Options:\n",
    "\n",
    "1. Variance is directly tied to exploration, with greater variance leading to more random actions and more exploration. \n",
    "2.  Learned variance always leads to better results than constant variance. \n",
    "3.  Constant variance is a simple solution that guarantees a certain level of exploration throughout training. \n",
    "4.  If constant variance is set too low, it can result in faster convergence to the optimal policy. \n",
    "5.  If the constant variance is set too high, it can handle very challenging environments by seeing through the randomness in selected actions.\n",
    "6.  Learning the variance allows the agent to adapt its exploration needs throughout training. \n",
    "7. Constant variance makes it possible for the agent to explore more at the beginning and less as the policy improves. \n",
    "8. Learning the variance can potentially reduce variance when actions are good and increase it when actions are bad. \n",
    "9. Understanding how learned variance behaves during optimization can be challenging.\n",
    "10. Constant variance is computationally more expensive than learned variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eb71f9-232a-4ab7-97e3-31972a7cb343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer question 2.1 with appropriate option numbers\n",
    "sq2_1 = [] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296320f9-3b14-4edc-b5f2-597a094e171d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d13eefd0d63d1ff18a00323cef460153",
     "grade": false,
     "grade_id": "cell-4cf801eac92a3b19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "The following cells are used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2333a4c-697c-4875-9885-e63284af3f2c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "683b1f0671b478fec843b6e7163a15ca",
     "grade": true,
     "grade_id": "cell-ae37e0b98565323f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d053c6e-6e65-4aec-ae9d-45ddb4cc3f13",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae84c5dba0956a86e0f1a806c86e0af1",
     "grade": true,
     "grade_id": "cell-a72089ecec4f5632",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71edfd5d-e17f-4eda-a717-9e44120af0e1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c189d5b475c6cced735a5314611c4f21",
     "grade": true,
     "grade_id": "cell-9e5f8847acea74a7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55b5584-6503-4fdb-8114-cc405a144043",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "115d571744e4b698680e42ef44a6a9f1",
     "grade": true,
     "grade_id": "cell-2be5408389da2f22",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5d5f33-0196-4a05-ab15-c14611e24329",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bff3925d27a08b1abb8436e8f9a2087a",
     "grade": true,
     "grade_id": "cell-e990636f285b1fc3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f701ff9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c782d710199b9b152f240bc37bc10069",
     "grade": false,
     "grade_id": "cell-e68f1d1241b5f955",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Q3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 2.2</b> Learnable Variance Initial Performance (5 points) </h3> \n",
    "\n",
    "In the case of learned variance, what are the impacts of initialization on the training performance? **Select 4 options.**\n",
    "\n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ffb191-91ed-411a-84db-cdba261fabb1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5d5279967303ed7150ab5b034687350",
     "grade": false,
     "grade_id": "cell-0b76bf2aa4735495",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Options:\n",
    "    \n",
    "1. A larger initial value allows for more exploration at the beginning of training. \n",
    "2. A lower initial value tends to favor exploitation over exploration initially. \n",
    "3. The initial variance value has no impact on the final performance of the trained policy. \n",
    "4. If the initial value is too low or too high, training can be slower and may require more data. \n",
    "5. A high initial variance leads to better final performance regardless of the task. \n",
    "6. The impact of the initial variance diminishes as training progresses and the variance is learned. \n",
    "7. The optimal initial variance can be theoretically calculated for any given task. \n",
    "8. If the variance decreases too rapidly, the optimizer might become trapped in a local minimum.\n",
    "9. Changing the initial variance is equivalent to adjusting the learning rate of the optimizer. \n",
    "10. The initial variance affects only the beginning of training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffebac0-8432-44a4-bfde-7b985e21fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer question 2.2 with appropriate option numbers\n",
    "sq2_2 = [] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be1c98b-8e1a-45a0-988d-d9b386a24f39",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d825ca8e2bb7a90eac70fa0a503f7cb4",
     "grade": false,
     "grade_id": "cell-76f57b4b47186121",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "The following cells are used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fc11d0-fd01-463d-8283-5af574797c1d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f38b6c0f01a00ce34763c3d48b24ca73",
     "grade": true,
     "grade_id": "cell-09b718ecfbe32325",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2709dfb7-56be-495d-803e-852110f47093",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a50c0ba4f2bf587aa935a79073d06e1",
     "grade": true,
     "grade_id": "cell-763f34edc660252e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff6107a-023d-461e-968d-1e17449f64d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32452fdda00d718cc0baaa7fb394651e",
     "grade": true,
     "grade_id": "cell-98c29f8b68e66232",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bbc19b-e340-42d0-a7ee-ab05a2ad92b8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5785645c12767d416dc6ddfed99efa21",
     "grade": true,
     "grade_id": "cell-9c9e78138da8d51d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e341103b",
   "metadata": {},
   "source": [
    "# 4. PG and experience replay <a id='4.'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6112bc95",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a68688c7ef8e04344930af37d7c63f2",
     "grade": false,
     "grade_id": "cell-6cb5eeda51f1d4eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='Q4'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 3.</b> Considering an experience replay buffer (15 points) </h3> \n",
    "\n",
    "Why can't the REINFORCE method implemented in this exercise be directly used with experience replay? Which steps of the algorithm would be problematic, and how could these issues potentially be resolved? **Select 4 options.**\n",
    "\n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0ea7f0-69a6-47f8-a46a-bb010300d0e1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "698fb1e0819100c4f301814e570244df",
     "grade": false,
     "grade_id": "cell-adb8f569068522a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Options:\n",
    "\n",
    "1.  REINFORCE is inherently off-policy and can be directly used with experience replay without modifications. \n",
    "2.  REINFORCE is an on-policy method, making it incompatible with off-policy data from a replay buffer. \n",
    "3. The problematic step is in the policy evaluation process, as the stored rewards may not reflect the current policy's performance.\n",
    "4.  The problem can be mitigated by frequently updating the replay buffer with experiences from more recent policies.\n",
    "5.  The expectation of the gradient must be calculated over the current policy in REINFORCE. \n",
    "6.  REINFORCE requires sequential data, which is disrupted by the random sampling in experience replay. \n",
    "7.  Computing the expectation term is difficult with experiences from policies other than the current one. \n",
    "8.  There is no way to use REINFORCE with experience replay under any circumstances.\n",
    "9.  Importance sampling can be used to estimate the gradients and potentially resolve the issue. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d2bf15-930e-4689-898b-d6a529174d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer question 3 with appropriate option numbers\n",
    "sq3 = [] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c820539-91be-40c3-9721-f3aecf0d7c71",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f9f72a20005df02b60eacdf9cd800a1",
     "grade": false,
     "grade_id": "cell-a7b8cbb0c7ee79bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "The following cells are used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a395615e-0977-42fc-97b2-0067d9daf743",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4413fbf9a4117123de48bdff40d8edb5",
     "grade": true,
     "grade_id": "cell-888f793f50035df5",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f6467d-1b3e-4990-8d6b-5710d8f43cd4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82a27507e21e239487f9dbc71b70113f",
     "grade": true,
     "grade_id": "cell-c1e5b1ebabd15135",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2e6daf-15bb-44b9-8613-ad1c420f248a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3151594ac1e83351cd877b808e0b3c67",
     "grade": true,
     "grade_id": "cell-a4e0e03a88560f44",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8de0c6-3e96-477f-a6f2-f14ba043f29d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb61e4be1d3786c02d82e25aa17d5550",
     "grade": true,
     "grade_id": "cell-89819ab91b6882b0",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7b67d36",
   "metadata": {},
   "source": [
    "# 5. Real-world control problems <a id='5.'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3731e57a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0ba52a4c04c3e5a61b1b086f6e08642",
     "grade": false,
     "grade_id": "cell-4cf495dce20c4b84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Q5'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 4.1</b> Considering an unbounded continuous action space part 1 (5 points) </h3> \n",
    "\n",
    "What could go wrong when a model with an **unbounded** continuous action space and a reward function like the one used here (+1 for survival) were to be used with a physical system? **Select 5 options.**\n",
    "\n",
    "\n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490748e0-4af6-46c7-8c1f-bd49b17b4988",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55f365842c25299d768ad29135696fc7",
     "grade": false,
     "grade_id": "cell-bd5793f602cb1128",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Options:\n",
    "    \n",
    "1. The agent would always converge to the most energy-efficient actions to ensure long-term survival. \n",
    "2. The agent might take extreme actions that could damage the physical hardware. \n",
    "3. The simple reward function would lead to slow learning but no physical dangers. \n",
    "4.  The model would naturally avoid extreme actions due to the survival instinct encoded in the reward. \n",
    "5.  The model could apply unrealistically large inputs (e.g., voltages) to system components. \n",
    "6.  The main issue would be the computational cost of handling unlimited actions, not physical dangers.\n",
    "7.  The agent might cause damage to the surroundings of the physical system. \n",
    "8.  The unbounded action space could lead to unsafe operating conditions for the system. \n",
    "9.  The model might learn to exploit the reward function by finding unintended shortcuts to survival. \n",
    "10.  Unbounded continuous action spaces guarantee optimal performance in physical systems. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097190c7-adc7-4ffa-9acb-600981bdfe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer question 4.1 with appropriate option numbers\n",
    "sq4_1 = [] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1502a88-ef13-4b1b-b258-b8f79173b5f2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "638469d35f910bd89d7923f1d4d54b79",
     "grade": false,
     "grade_id": "cell-b43da332879b800b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "The following cells are used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8826d1-9986-46e4-85e5-cf27196e83e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "546aecc1550081b14b1522050105c6bf",
     "grade": true,
     "grade_id": "cell-d5a8ec81fd0d66b1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479588a-4204-4439-a472-3f42e28a074c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4aa68a722fdde09a0f14b1cc2beca952",
     "grade": true,
     "grade_id": "cell-dd21e01105b76141",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ddda5c-62d7-4972-8499-b366c33180d9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86c6d11ddefd70ca91ca251f95336b79",
     "grade": true,
     "grade_id": "cell-1fcee280857bec9d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be07e454-ef17-49f7-9bb6-bc8cea0f33b1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c33a96b244d47f6d295480d7c250f1e",
     "grade": true,
     "grade_id": "cell-8047c00c583ed0b5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aa6875-27b5-4493-9bbc-947210f76997",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3568917e141d50f586181ced3a278c15",
     "grade": true,
     "grade_id": "cell-bd91fc42a27e31c6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb45baf8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67b760375ed0a6a2b94dfb07ea65d28e",
     "grade": false,
     "grade_id": "cell-7efb2faf02c622ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Q6'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 4.2</b> Considering an unbounded continuous action space part 2 (10 points) </h3> \n",
    "\n",
    "How could the problems appearing in Question 4.1 be mitigated without putting a hard limit on the actions? **Select 3 options.**\n",
    "\n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd8de8b-0921-4e92-99a7-bf716b05d958",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "27fc63402758fa9596a3e25808be10f6",
     "grade": false,
     "grade_id": "cell-aa619f6370139c43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Options:\n",
    "    \n",
    "1. Only need to add a cost for constraining the state-space of the system\n",
    "2. Implement a reward function with a cost on extreme actions. \n",
    "3. Increase the frequency of agent updates to react more quickly to potential dangers.\n",
    "4.  Penalize actions that lead to unsafe states in the reward function. \n",
    "5.  Train the agent for more episodes to ensure it learns to avoid dangerous actions.\n",
    "6.  Modify the reward function to encourage safer behavior. \n",
    "7.  Increase the exploration rate to ensure the agent finds safe actions more quickly.\n",
    "8. Use a larger neural network to better approximate the optimal policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7a8af1-1828-430d-8c93-b066c8c06290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer question 4.2 with appropriate option numbers\n",
    "sq4_2 = [] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7180cb44-e7f8-46c3-a998-82b8ca735e52",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6805e89aea5be8794bf596d54fa3a4b",
     "grade": false,
     "grade_id": "cell-3b450e8fc52b5fde",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "The following cells are used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91768e7-c369-4f8b-8775-05b643ca6a18",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3652e018ac01e53271b7629acbc23b13",
     "grade": true,
     "grade_id": "cell-f9c4479f99666417",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ebbea3-f23d-401e-8aeb-66509850c935",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e999e719e7e4b1c084cb7824d3d757a",
     "grade": true,
     "grade_id": "cell-25d8dcd46c0a0aad",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c207fd7-f8ad-4faf-a3e6-bae40bce0ae0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8f37ba4a5b559d2961d11bccb39f025",
     "grade": true,
     "grade_id": "cell-ad280a1aceb1fe48",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0eacb159",
   "metadata": {},
   "source": [
    "# 6. Discrete action spaces <a id='6.'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078ceb81",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66dcd9987ec7c4e5ebda97a81438d820",
     "grade": false,
     "grade_id": "cell-ef7074d497b36665",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='Q7'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Question 5.</b> Considering discrete action spaces (10 points) </h3> \n",
    "\n",
    "Can policy gradient methods be used with discrete action spaces? If so, how would they be implemented? **Select 5 options.**\n",
    "    \n",
    "üîù\t<a href='#TOC'><b>Table of Contents</b></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d9c3ec-d223-452d-8a47-4f8073e3066a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "26f3c4738d927b2de66b599a486ba532",
     "grade": false,
     "grade_id": "cell-7f1aac4a2b0a31e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Options:\n",
    "    \n",
    "1. Yes, policy gradient methods can be used with discrete action spaces. \n",
    "2. The gradient can still be computed and used to update the policy in discrete action spaces. \n",
    "3. Policy gradient methods are fundamentally incompatible with discrete action spaces. \n",
    "4. In discrete action spaces, the policy network would output probabilities for each action instead of means and variances. \n",
    "5. No changes to the algorithm are needed; it works identically for continuous and discrete spaces. \n",
    "6. A categorical (softmax) distribution is typically used for the policy in discrete action spaces. \n",
    "7. In discrete spaces, the policy gradient is computed using the log-probability of the chosen action. \n",
    "8. Discrete action spaces require a completely different class of algorithms and cannot use policy gradients. \n",
    "9. The main challenge is in defining a continuous approximation of the discrete action space. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd5abb-dd0a-43ea-b413-d2ce26fdab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer question 5 with appropriate option numbers\n",
    "sq5 = [] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a457fc65-a43e-4bc2-ac20-6f5304cf9dfa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dce09cff7bd5d1047689ababaef90649",
     "grade": false,
     "grade_id": "cell-93982c40108e75aa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "The following cells are used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae4646c-26ee-456b-a791-ffdaf1dea484",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae1a9141639c916b8df9810c7041bb4b",
     "grade": true,
     "grade_id": "cell-455629eb8ff429fc",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15dc545-4698-489b-a49c-3413cf70a548",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b21d5d93a5b584d204404eb34b889116",
     "grade": true,
     "grade_id": "cell-ccd64746d61c21fe",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa9a214-e0c3-42c8-bba9-28898f3a9758",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b411904077d2218347dba63d31cc773",
     "grade": true,
     "grade_id": "cell-3b1379631af22fbe",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00222dd3-06b1-436c-93f4-f6b60147bb2a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86b3c8b0c452ce715cc376a7b4172974",
     "grade": true,
     "grade_id": "cell-a58c487d6d744df5",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c97890-ddd1-4824-bb2b-2e5be84291f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ed761b8c0d3a85b2b576a6e3ca8296a",
     "grade": true,
     "grade_id": "cell-6f7c8a46592ca476",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c50da4d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9965fc6e7758f8a420963a52b936f443",
     "grade": false,
     "grade_id": "cell-8d4be4c9092a091c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# 7. Submitting <a id='7.'></a>\n",
    "\n",
    "Ensure all tasks and questions in `ex5.ipynb` are thoroughly answered and that all the relevant plots are displayed correctly (there is no need to save the plots in the results folder). \n",
    "\n",
    "The training performance plots are as follows:\n",
    "\n",
    "- **Task 1a, REINFORCE without baseline**: Ensure the implementation is correct and the plot is similar to the expected plot shown in the task description. Refer to [Figure 1a](#figure-1a).\n",
    "  \n",
    "- **Task 1b, REINFORCE with constant baseline b = 20**: Properly implement the algorithm with a constant baseline and plot the training performance. Refer to [Figure 1b](#figure-1b).\n",
    "  \n",
    "- **Task 1c, REINFORCE with discounted rewards normalized to zero mean and unit variance**: Implement and plot the performance ensuring the rewards are normalized correctly. Refer to [Figure 1c](#figure-1c).\n",
    "  \n",
    "- **Task 2, REINFORCE with learned variance**: Ensure that the variance is being learned and adjusted during training, and plot the training performance.\n",
    "\n",
    "No model files need to be saved for this assignment.\n",
    "\n",
    "üîù [Back to Table of Contents](#TOC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a8428",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f2a35dca22d4e6a91f7259550809999",
     "grade": true,
     "grade_id": "cell-26f0fb23212021f6",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Make sure that skip training is set to True before submission\n",
    "assert skip_training == True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad098b5f",
   "metadata": {},
   "source": [
    "## 7.1 Feedback <a id='7.1'></a>\n",
    "\n",
    "In order to help the staff of the course as well as the forthcoming students, it would be great if you could answer to the following questions in your submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed5c6d1-c83a-4d62-9b4f-5ed33f0f6aea",
   "metadata": {},
   "source": [
    "1) How much time did you spend solving this exercise? (change the ```hrs``` variable below to a floating point number representing the number of hours taken e.g. 5.43)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216bc97b-e49b-4c18-8c41-1c45711b90cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hrs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d26d7a7-acc4-48db-84a8-e366ea68a709",
   "metadata": {},
   "source": [
    "2) Difficulty of each task/question from 1-5 (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeb90fa-9f33-49c1-aca3-55f9a9a00d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "T1a = None   # Task 1a. basic REINFORCE without baseline (15 points)\n",
    "T1b = None   # Task 1b. REINFORCE with a constant baseline b = 20 (5 points)\n",
    "T1c = None   # Task 1c. REINFORCE with discounted rewards normalized to zero mean and unit variance (5 points)\n",
    "Q1_1 = None  # Question 1.1 A good baseline (15 points)\n",
    "T2 = None    # Task 2. Making Variance a Learnable Parameter (10 points)\n",
    "Q2_1 = None  # Question 2.1 Constant vs Learnable Variance (5 points)\n",
    "Q2_2 = None  # Question 2.2 Learnable Variance Initial Performance (5 points)\n",
    "Q3 = None    # Question 3. Considering a experience reply buffer (15 points)\n",
    "Q4_1 = None  # Question 4.1 Considering an unbounded continuous action space part 1 (5 points)\n",
    "Q4_2 = None  # Question 4.2 Considering an unbounded continuous action space part 2 (10 points)\n",
    "Q5 = None    # Question 5. Considering discrete action spaces (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9387c6-00de-4414-86eb-801485a59ec7",
   "metadata": {},
   "source": [
    "3) How well did you understand the content of the task/question from 1-5? (int or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a53120-fcf2-4dbf-b6ad-87e5aff7dee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "T1a = None   # Task 1a. basic REINFORCE without baseline (15 points)\n",
    "T1b = None   # Task 1b. REINFORCE with a constant baseline b = 20 (5 points)\n",
    "T1c = None   # Task 1c. REINFORCE with discounted rewards normalized to zero mean and unit variance (5 points)\n",
    "Q1_1 = None  # Question 1.1 A good baseline (15 points)\n",
    "T2 = None    # Task 2. Making Variance a Learnable Parameter (10 points)\n",
    "Q2_1 = None  # Question 2.1 Constant vs Learnable Variance (5 points)\n",
    "Q2_2 = None  # Question 2.2 Learnable Variance Initial Performance (5 points)\n",
    "Q3 = None    # Question 3. Considering a experience reply buffer (15 points)\n",
    "Q4_1 = None  # Question 4.1 Considering an unbounded continuous action space part 1 (5 points)\n",
    "Q4_2 = None  # Question 4.2 Considering an unbounded continuous action space part 2 (10 points)\n",
    "Q5 = None    # Question 5. Considering discrete action spaces (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7595057e-8666-486a-bd8c-788b4de828a7",
   "metadata": {},
   "source": [
    "4) General feedback. Consider questions like:\n",
    "\n",
    "    - Did the content of the lecture relate well with the assignment?\n",
    "    - To what extent did you find the material to be potentially useful for your research and studies?\n",
    "    \n",
    "And other feedback you think is worth including. Type in the box below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e53a93d-a13e-4ced-bf49-2c5b406b8653",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOUBLE CLICK HERE TO EDIT, CLEAR THIS TEXT AND ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f5f8d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# References <a id='8.'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
